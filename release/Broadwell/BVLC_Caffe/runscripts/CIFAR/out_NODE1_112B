I0328 15:03:53.591852 163330 caffe.cpp:210] Use CPU.
I0328 15:03:53.592842 163330 solver.cpp:48] Initializing solver from parameters: 
test_iter: 0
test_interval: 2000
base_lr: 0.001
display: 800
max_iter: 800
lr_policy: "fixed"
momentum: 0.9
weight_decay: 0.004
snapshot: 10000
snapshot_prefix: "examples/cifar10/cifar10_full"
solver_mode: CPU
net: "examples/cifar10/cifar10_full_train_test_bsize112.prototxt"
train_state {
  level: 0
  stage: ""
}
snapshot_after_train: false
snapshot_format: HDF5
I0328 15:03:53.593164 163330 solver.cpp:91] Creating training net from net file: examples/cifar10/cifar10_full_train_test_bsize112.prototxt
I0328 15:03:53.594164 163330 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer cifar
I0328 15:03:53.594205 163330 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0328 15:03:53.594521 163330 net.cpp:58] Initializing net from parameters: 
name: "CIFAR10_full"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "cifar"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mean_file: "examples/cifar10/mean.binaryproto"
  }
  data_param {
    source: "examples/cifar10/cifar10_train_lmdb"
    batch_size: 112
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 32
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.0001
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "pool1"
  top: "pool1"
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "pool1"
  top: "norm1"
  lrn_param {
    local_size: 3
    alpha: 5e-05
    beta: 0.75
    norm_region: WITHIN_CHANNEL
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "norm1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 32
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "pool2"
  top: "norm2"
  lrn_param {
    local_size: 3
    alpha: 5e-05
    beta: 0.75
    norm_region: WITHIN_CHANNEL
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "norm2"
  top: "conv3"
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool3"
  top: "ip1"
  param {
    lr_mult: 1
    decay_mult: 250
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip1"
  bottom: "label"
  top: "loss"
}
I0328 15:03:53.595082 163330 layer_factory.hpp:77] Creating layer cifar
I0328 15:03:53.596174 163330 net.cpp:100] Creating Layer cifar
I0328 15:03:53.596215 163330 net.cpp:408] cifar -> data
I0328 15:03:53.596350 163331 db_lmdb.cpp:35] Opened lmdb examples/cifar10/cifar10_train_lmdb
I0328 15:03:53.596356 163330 net.cpp:408] cifar -> label
I0328 15:03:53.596428 163330 data_transformer.cpp:25] Loading mean file from: examples/cifar10/mean.binaryproto
I0328 15:03:53.596789 163330 data_layer.cpp:41] output data size: 112,3,32,32
I0328 15:03:53.601280 163330 net.cpp:150] Setting up cifar
I0328 15:03:53.601321 163330 net.cpp:157] Top shape: 112 3 32 32 (344064)
I0328 15:03:53.601331 163330 net.cpp:157] Top shape: 112 (112)
I0328 15:03:53.601336 163330 net.cpp:165] Memory required for data: 1376704
I0328 15:03:53.601354 163330 layer_factory.hpp:77] Creating layer conv1
I0328 15:03:53.601400 163330 net.cpp:100] Creating Layer conv1
I0328 15:03:53.601419 163330 net.cpp:434] conv1 <- data
I0328 15:03:53.601461 163330 net.cpp:408] conv1 -> conv1
I0328 15:03:53.602247 163330 net.cpp:150] Setting up conv1
I0328 15:03:53.602267 163330 net.cpp:157] Top shape: 112 32 32 32 (3670016)
I0328 15:03:53.602272 163330 net.cpp:165] Memory required for data: 16056768
I0328 15:03:53.602329 163330 layer_factory.hpp:77] Creating layer pool1
I0328 15:03:53.602360 163330 net.cpp:100] Creating Layer pool1
I0328 15:03:53.602368 163330 net.cpp:434] pool1 <- conv1
I0328 15:03:53.602386 163330 net.cpp:408] pool1 -> pool1
I0328 15:03:53.602432 163330 net.cpp:150] Setting up pool1
I0328 15:03:53.602445 163330 net.cpp:157] Top shape: 112 32 16 16 (917504)
I0328 15:03:53.602450 163330 net.cpp:165] Memory required for data: 19726784
I0328 15:03:53.602457 163330 layer_factory.hpp:77] Creating layer relu1
I0328 15:03:53.602478 163330 net.cpp:100] Creating Layer relu1
I0328 15:03:53.602485 163330 net.cpp:434] relu1 <- pool1
I0328 15:03:53.602500 163330 net.cpp:395] relu1 -> pool1 (in-place)
I0328 15:03:53.602516 163330 net.cpp:150] Setting up relu1
I0328 15:03:53.602524 163330 net.cpp:157] Top shape: 112 32 16 16 (917504)
I0328 15:03:53.602529 163330 net.cpp:165] Memory required for data: 23396800
I0328 15:03:53.602535 163330 layer_factory.hpp:77] Creating layer norm1
I0328 15:03:53.602563 163330 net.cpp:100] Creating Layer norm1
I0328 15:03:53.602571 163330 net.cpp:434] norm1 <- pool1
I0328 15:03:53.602587 163330 net.cpp:408] norm1 -> norm1
I0328 15:03:53.602756 163330 net.cpp:150] Setting up norm1
I0328 15:03:53.602774 163330 net.cpp:157] Top shape: 112 32 16 16 (917504)
I0328 15:03:53.602778 163330 net.cpp:165] Memory required for data: 27066816
I0328 15:03:53.602787 163330 layer_factory.hpp:77] Creating layer conv2
I0328 15:03:53.602813 163330 net.cpp:100] Creating Layer conv2
I0328 15:03:53.602823 163330 net.cpp:434] conv2 <- norm1
I0328 15:03:53.602847 163330 net.cpp:408] conv2 -> conv2
I0328 15:03:53.609098 163330 net.cpp:150] Setting up conv2
I0328 15:03:53.609117 163330 net.cpp:157] Top shape: 112 32 16 16 (917504)
I0328 15:03:53.609120 163330 net.cpp:165] Memory required for data: 30736832
I0328 15:03:53.609148 163330 layer_factory.hpp:77] Creating layer relu2
I0328 15:03:53.609169 163330 net.cpp:100] Creating Layer relu2
I0328 15:03:53.609175 163330 net.cpp:434] relu2 <- conv2
I0328 15:03:53.609191 163330 net.cpp:395] relu2 -> conv2 (in-place)
I0328 15:03:53.609208 163330 net.cpp:150] Setting up relu2
I0328 15:03:53.609215 163330 net.cpp:157] Top shape: 112 32 16 16 (917504)
I0328 15:03:53.609220 163330 net.cpp:165] Memory required for data: 34406848
I0328 15:03:53.609226 163330 layer_factory.hpp:77] Creating layer pool2
I0328 15:03:53.609246 163330 net.cpp:100] Creating Layer pool2
I0328 15:03:53.609252 163330 net.cpp:434] pool2 <- conv2
I0328 15:03:53.609268 163330 net.cpp:408] pool2 -> pool2
I0328 15:03:53.609297 163330 net.cpp:150] Setting up pool2
I0328 15:03:53.609307 163330 net.cpp:157] Top shape: 112 32 8 8 (229376)
I0328 15:03:53.609311 163330 net.cpp:165] Memory required for data: 35324352
I0328 15:03:53.609318 163330 layer_factory.hpp:77] Creating layer norm2
I0328 15:03:53.609341 163330 net.cpp:100] Creating Layer norm2
I0328 15:03:53.609349 163330 net.cpp:434] norm2 <- pool2
I0328 15:03:53.609369 163330 net.cpp:408] norm2 -> norm2
I0328 15:03:53.609452 163330 net.cpp:150] Setting up norm2
I0328 15:03:53.609467 163330 net.cpp:157] Top shape: 112 32 8 8 (229376)
I0328 15:03:53.609470 163330 net.cpp:165] Memory required for data: 36241856
I0328 15:03:53.609483 163330 layer_factory.hpp:77] Creating layer conv3
I0328 15:03:53.609519 163330 net.cpp:100] Creating Layer conv3
I0328 15:03:53.609529 163330 net.cpp:434] conv3 <- norm2
I0328 15:03:53.609552 163330 net.cpp:408] conv3 -> conv3
I0328 15:03:53.621978 163330 net.cpp:150] Setting up conv3
I0328 15:03:53.621997 163330 net.cpp:157] Top shape: 112 64 8 8 (458752)
I0328 15:03:53.622001 163330 net.cpp:165] Memory required for data: 38076864
I0328 15:03:53.622030 163330 layer_factory.hpp:77] Creating layer relu3
I0328 15:03:53.622048 163330 net.cpp:100] Creating Layer relu3
I0328 15:03:53.622056 163330 net.cpp:434] relu3 <- conv3
I0328 15:03:53.622072 163330 net.cpp:395] relu3 -> conv3 (in-place)
I0328 15:03:53.622089 163330 net.cpp:150] Setting up relu3
I0328 15:03:53.622097 163330 net.cpp:157] Top shape: 112 64 8 8 (458752)
I0328 15:03:53.622102 163330 net.cpp:165] Memory required for data: 39911872
I0328 15:03:53.622107 163330 layer_factory.hpp:77] Creating layer pool3
I0328 15:03:53.622122 163330 net.cpp:100] Creating Layer pool3
I0328 15:03:53.622128 163330 net.cpp:434] pool3 <- conv3
I0328 15:03:53.622143 163330 net.cpp:408] pool3 -> pool3
I0328 15:03:53.622169 163330 net.cpp:150] Setting up pool3
I0328 15:03:53.622179 163330 net.cpp:157] Top shape: 112 64 4 4 (114688)
I0328 15:03:53.622184 163330 net.cpp:165] Memory required for data: 40370624
I0328 15:03:53.622190 163330 layer_factory.hpp:77] Creating layer ip1
I0328 15:03:53.622215 163330 net.cpp:100] Creating Layer ip1
I0328 15:03:53.622223 163330 net.cpp:434] ip1 <- pool3
I0328 15:03:53.622242 163330 net.cpp:408] ip1 -> ip1
I0328 15:03:53.624774 163330 net.cpp:150] Setting up ip1
I0328 15:03:53.624790 163330 net.cpp:157] Top shape: 112 10 (1120)
I0328 15:03:53.624794 163330 net.cpp:165] Memory required for data: 40375104
I0328 15:03:53.624809 163330 layer_factory.hpp:77] Creating layer loss
I0328 15:03:53.624833 163330 net.cpp:100] Creating Layer loss
I0328 15:03:53.624841 163330 net.cpp:434] loss <- ip1
I0328 15:03:53.624855 163330 net.cpp:434] loss <- label
I0328 15:03:53.624873 163330 net.cpp:408] loss -> loss
I0328 15:03:53.624897 163330 layer_factory.hpp:77] Creating layer loss
I0328 15:03:53.624959 163330 net.cpp:150] Setting up loss
I0328 15:03:53.624969 163330 net.cpp:157] Top shape: (1)
I0328 15:03:53.624974 163330 net.cpp:160]     with loss weight 1
I0328 15:03:53.624992 163330 net.cpp:165] Memory required for data: 40375108
I0328 15:03:53.625000 163330 net.cpp:226] loss needs backward computation.
I0328 15:03:53.625010 163330 net.cpp:226] ip1 needs backward computation.
I0328 15:03:53.625015 163330 net.cpp:226] pool3 needs backward computation.
I0328 15:03:53.625020 163330 net.cpp:226] relu3 needs backward computation.
I0328 15:03:53.625025 163330 net.cpp:226] conv3 needs backward computation.
I0328 15:03:53.625031 163330 net.cpp:226] norm2 needs backward computation.
I0328 15:03:53.625036 163330 net.cpp:226] pool2 needs backward computation.
I0328 15:03:53.625042 163330 net.cpp:226] relu2 needs backward computation.
I0328 15:03:53.625047 163330 net.cpp:226] conv2 needs backward computation.
I0328 15:03:53.625052 163330 net.cpp:226] norm1 needs backward computation.
I0328 15:03:53.625057 163330 net.cpp:226] relu1 needs backward computation.
I0328 15:03:53.625062 163330 net.cpp:226] pool1 needs backward computation.
I0328 15:03:53.625067 163330 net.cpp:226] conv1 needs backward computation.
I0328 15:03:53.625074 163330 net.cpp:228] cifar does not need backward computation.
I0328 15:03:53.625082 163330 net.cpp:270] This network produces output loss
I0328 15:03:53.625116 163330 net.cpp:283] Network initialization done.
I0328 15:03:53.625874 163330 solver.cpp:181] Creating test net (#0) specified by net file: examples/cifar10/cifar10_full_train_test_bsize112.prototxt
I0328 15:03:53.625957 163330 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer cifar
I0328 15:03:53.626289 163330 net.cpp:58] Initializing net from parameters: 
name: "CIFAR10_full"
state {
  phase: TEST
}
layer {
  name: "cifar"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mean_file: "examples/cifar10/mean.binaryproto"
  }
  data_param {
    source: "examples/cifar10/cifar10_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 32
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.0001
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "pool1"
  top: "pool1"
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "pool1"
  top: "norm1"
  lrn_param {
    local_size: 3
    alpha: 5e-05
    beta: 0.75
    norm_region: WITHIN_CHANNEL
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "norm1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 32
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "pool2"
  top: "norm2"
  lrn_param {
    local_size: 3
    alpha: 5e-05
    beta: 0.75
    norm_region: WITHIN_CHANNEL
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "norm2"
  top: "conv3"
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool3"
  top: "ip1"
  param {
    lr_mult: 1
    decay_mult: 250
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip1"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip1"
  bottom: "label"
  top: "loss"
}
I0328 15:03:53.626602 163330 layer_factory.hpp:77] Creating layer cifar
I0328 15:03:53.626857 163330 net.cpp:100] Creating Layer cifar
I0328 15:03:53.626883 163330 net.cpp:408] cifar -> data
I0328 15:03:53.626910 163333 db_lmdb.cpp:35] Opened lmdb examples/cifar10/cifar10_test_lmdb
I0328 15:03:53.626919 163330 net.cpp:408] cifar -> label
I0328 15:03:53.626942 163330 data_transformer.cpp:25] Loading mean file from: examples/cifar10/mean.binaryproto
I0328 15:03:53.627193 163330 data_layer.cpp:41] output data size: 100,3,32,32
I0328 15:03:53.631160 163330 net.cpp:150] Setting up cifar
I0328 15:03:53.631181 163330 net.cpp:157] Top shape: 100 3 32 32 (307200)
I0328 15:03:53.631188 163330 net.cpp:157] Top shape: 100 (100)
I0328 15:03:53.631192 163330 net.cpp:165] Memory required for data: 1229200
I0328 15:03:53.631204 163330 layer_factory.hpp:77] Creating layer label_cifar_1_split
I0328 15:03:53.631229 163330 net.cpp:100] Creating Layer label_cifar_1_split
I0328 15:03:53.631237 163330 net.cpp:434] label_cifar_1_split <- label
I0328 15:03:53.631256 163330 net.cpp:408] label_cifar_1_split -> label_cifar_1_split_0
I0328 15:03:53.631283 163330 net.cpp:408] label_cifar_1_split -> label_cifar_1_split_1
I0328 15:03:53.631306 163330 net.cpp:150] Setting up label_cifar_1_split
I0328 15:03:53.631323 163330 net.cpp:157] Top shape: 100 (100)
I0328 15:03:53.631338 163330 net.cpp:157] Top shape: 100 (100)
I0328 15:03:53.631343 163330 net.cpp:165] Memory required for data: 1230000
I0328 15:03:53.631350 163330 layer_factory.hpp:77] Creating layer conv1
I0328 15:03:53.631377 163330 net.cpp:100] Creating Layer conv1
I0328 15:03:53.631386 163330 net.cpp:434] conv1 <- data
I0328 15:03:53.631407 163330 net.cpp:408] conv1 -> conv1
I0328 15:03:53.632077 163330 net.cpp:150] Setting up conv1
I0328 15:03:53.632095 163330 net.cpp:157] Top shape: 100 32 32 32 (3276800)
I0328 15:03:53.632100 163330 net.cpp:165] Memory required for data: 14337200
I0328 15:03:53.632136 163330 layer_factory.hpp:77] Creating layer pool1
I0328 15:03:53.632158 163330 net.cpp:100] Creating Layer pool1
I0328 15:03:53.632165 163330 net.cpp:434] pool1 <- conv1
I0328 15:03:53.632182 163330 net.cpp:408] pool1 -> pool1
I0328 15:03:53.632215 163330 net.cpp:150] Setting up pool1
I0328 15:03:53.632225 163330 net.cpp:157] Top shape: 100 32 16 16 (819200)
I0328 15:03:53.632230 163330 net.cpp:165] Memory required for data: 17614000
I0328 15:03:53.632236 163330 layer_factory.hpp:77] Creating layer relu1
I0328 15:03:53.632254 163330 net.cpp:100] Creating Layer relu1
I0328 15:03:53.632261 163330 net.cpp:434] relu1 <- pool1
I0328 15:03:53.632279 163330 net.cpp:395] relu1 -> pool1 (in-place)
I0328 15:03:53.632295 163330 net.cpp:150] Setting up relu1
I0328 15:03:53.632303 163330 net.cpp:157] Top shape: 100 32 16 16 (819200)
I0328 15:03:53.632308 163330 net.cpp:165] Memory required for data: 20890800
I0328 15:03:53.632314 163330 layer_factory.hpp:77] Creating layer norm1
I0328 15:03:53.632330 163330 net.cpp:100] Creating Layer norm1
I0328 15:03:53.632338 163330 net.cpp:434] norm1 <- pool1
I0328 15:03:53.632352 163330 net.cpp:408] norm1 -> norm1
I0328 15:03:53.632436 163330 net.cpp:150] Setting up norm1
I0328 15:03:53.632449 163330 net.cpp:157] Top shape: 100 32 16 16 (819200)
I0328 15:03:53.632453 163330 net.cpp:165] Memory required for data: 24167600
I0328 15:03:53.632460 163330 layer_factory.hpp:77] Creating layer conv2
I0328 15:03:53.632486 163330 net.cpp:100] Creating Layer conv2
I0328 15:03:53.632495 163330 net.cpp:434] conv2 <- norm1
I0328 15:03:53.632519 163330 net.cpp:408] conv2 -> conv2
I0328 15:03:53.638770 163330 net.cpp:150] Setting up conv2
I0328 15:03:53.638787 163330 net.cpp:157] Top shape: 100 32 16 16 (819200)
I0328 15:03:53.638792 163330 net.cpp:165] Memory required for data: 27444400
I0328 15:03:53.638819 163330 layer_factory.hpp:77] Creating layer relu2
I0328 15:03:53.638839 163330 net.cpp:100] Creating Layer relu2
I0328 15:03:53.638847 163330 net.cpp:434] relu2 <- conv2
I0328 15:03:53.638864 163330 net.cpp:395] relu2 -> conv2 (in-place)
I0328 15:03:53.638880 163330 net.cpp:150] Setting up relu2
I0328 15:03:53.638887 163330 net.cpp:157] Top shape: 100 32 16 16 (819200)
I0328 15:03:53.638891 163330 net.cpp:165] Memory required for data: 30721200
I0328 15:03:53.638897 163330 layer_factory.hpp:77] Creating layer pool2
I0328 15:03:53.638916 163330 net.cpp:100] Creating Layer pool2
I0328 15:03:53.638923 163330 net.cpp:434] pool2 <- conv2
I0328 15:03:53.638942 163330 net.cpp:408] pool2 -> pool2
I0328 15:03:53.638974 163330 net.cpp:150] Setting up pool2
I0328 15:03:53.638986 163330 net.cpp:157] Top shape: 100 32 8 8 (204800)
I0328 15:03:53.638990 163330 net.cpp:165] Memory required for data: 31540400
I0328 15:03:53.638996 163330 layer_factory.hpp:77] Creating layer norm2
I0328 15:03:53.639017 163330 net.cpp:100] Creating Layer norm2
I0328 15:03:53.639025 163330 net.cpp:434] norm2 <- pool2
I0328 15:03:53.639042 163330 net.cpp:408] norm2 -> norm2
I0328 15:03:53.639127 163330 net.cpp:150] Setting up norm2
I0328 15:03:53.639140 163330 net.cpp:157] Top shape: 100 32 8 8 (204800)
I0328 15:03:53.639144 163330 net.cpp:165] Memory required for data: 32359600
I0328 15:03:53.639152 163330 layer_factory.hpp:77] Creating layer conv3
I0328 15:03:53.639176 163330 net.cpp:100] Creating Layer conv3
I0328 15:03:53.639185 163330 net.cpp:434] conv3 <- norm2
I0328 15:03:53.639214 163330 net.cpp:408] conv3 -> conv3
I0328 15:03:53.651672 163330 net.cpp:150] Setting up conv3
I0328 15:03:53.651690 163330 net.cpp:157] Top shape: 100 64 8 8 (409600)
I0328 15:03:53.651695 163330 net.cpp:165] Memory required for data: 33998000
I0328 15:03:53.651722 163330 layer_factory.hpp:77] Creating layer relu3
I0328 15:03:53.651739 163330 net.cpp:100] Creating Layer relu3
I0328 15:03:53.651747 163330 net.cpp:434] relu3 <- conv3
I0328 15:03:53.651767 163330 net.cpp:395] relu3 -> conv3 (in-place)
I0328 15:03:53.651783 163330 net.cpp:150] Setting up relu3
I0328 15:03:53.651792 163330 net.cpp:157] Top shape: 100 64 8 8 (409600)
I0328 15:03:53.651795 163330 net.cpp:165] Memory required for data: 35636400
I0328 15:03:53.651801 163330 layer_factory.hpp:77] Creating layer pool3
I0328 15:03:53.651816 163330 net.cpp:100] Creating Layer pool3
I0328 15:03:53.651823 163330 net.cpp:434] pool3 <- conv3
I0328 15:03:53.651839 163330 net.cpp:408] pool3 -> pool3
I0328 15:03:53.651865 163330 net.cpp:150] Setting up pool3
I0328 15:03:53.651875 163330 net.cpp:157] Top shape: 100 64 4 4 (102400)
I0328 15:03:53.651880 163330 net.cpp:165] Memory required for data: 36046000
I0328 15:03:53.651886 163330 layer_factory.hpp:77] Creating layer ip1
I0328 15:03:53.651909 163330 net.cpp:100] Creating Layer ip1
I0328 15:03:53.651917 163330 net.cpp:434] ip1 <- pool3
I0328 15:03:53.651940 163330 net.cpp:408] ip1 -> ip1
I0328 15:03:53.654448 163330 net.cpp:150] Setting up ip1
I0328 15:03:53.654465 163330 net.cpp:157] Top shape: 100 10 (1000)
I0328 15:03:53.654470 163330 net.cpp:165] Memory required for data: 36050000
I0328 15:03:53.654485 163330 layer_factory.hpp:77] Creating layer ip1_ip1_0_split
I0328 15:03:53.654505 163330 net.cpp:100] Creating Layer ip1_ip1_0_split
I0328 15:03:53.654512 163330 net.cpp:434] ip1_ip1_0_split <- ip1
I0328 15:03:53.654530 163330 net.cpp:408] ip1_ip1_0_split -> ip1_ip1_0_split_0
I0328 15:03:53.654547 163330 net.cpp:408] ip1_ip1_0_split -> ip1_ip1_0_split_1
I0328 15:03:53.654567 163330 net.cpp:150] Setting up ip1_ip1_0_split
I0328 15:03:53.654577 163330 net.cpp:157] Top shape: 100 10 (1000)
I0328 15:03:53.654582 163330 net.cpp:157] Top shape: 100 10 (1000)
I0328 15:03:53.654587 163330 net.cpp:165] Memory required for data: 36058000
I0328 15:03:53.654592 163330 layer_factory.hpp:77] Creating layer accuracy
I0328 15:03:53.654614 163330 net.cpp:100] Creating Layer accuracy
I0328 15:03:53.654623 163330 net.cpp:434] accuracy <- ip1_ip1_0_split_0
I0328 15:03:53.654644 163330 net.cpp:434] accuracy <- label_cifar_1_split_0
I0328 15:03:53.654660 163330 net.cpp:408] accuracy -> accuracy
I0328 15:03:53.654686 163330 net.cpp:150] Setting up accuracy
I0328 15:03:53.654696 163330 net.cpp:157] Top shape: (1)
I0328 15:03:53.654700 163330 net.cpp:165] Memory required for data: 36058004
I0328 15:03:53.654706 163330 layer_factory.hpp:77] Creating layer loss
I0328 15:03:53.654726 163330 net.cpp:100] Creating Layer loss
I0328 15:03:53.654733 163330 net.cpp:434] loss <- ip1_ip1_0_split_1
I0328 15:03:53.654747 163330 net.cpp:434] loss <- label_cifar_1_split_1
I0328 15:03:53.654760 163330 net.cpp:408] loss -> loss
I0328 15:03:53.654784 163330 layer_factory.hpp:77] Creating layer loss
I0328 15:03:53.654840 163330 net.cpp:150] Setting up loss
I0328 15:03:53.654850 163330 net.cpp:157] Top shape: (1)
I0328 15:03:53.654855 163330 net.cpp:160]     with loss weight 1
I0328 15:03:53.654862 163330 net.cpp:165] Memory required for data: 36058008
I0328 15:03:53.654870 163330 net.cpp:226] loss needs backward computation.
I0328 15:03:53.654880 163330 net.cpp:228] accuracy does not need backward computation.
I0328 15:03:53.654886 163330 net.cpp:226] ip1_ip1_0_split needs backward computation.
I0328 15:03:53.654891 163330 net.cpp:226] ip1 needs backward computation.
I0328 15:03:53.654897 163330 net.cpp:226] pool3 needs backward computation.
I0328 15:03:53.654902 163330 net.cpp:226] relu3 needs backward computation.
I0328 15:03:53.654907 163330 net.cpp:226] conv3 needs backward computation.
I0328 15:03:53.654913 163330 net.cpp:226] norm2 needs backward computation.
I0328 15:03:53.654924 163330 net.cpp:226] pool2 needs backward computation.
I0328 15:03:53.654938 163330 net.cpp:226] relu2 needs backward computation.
I0328 15:03:53.654943 163330 net.cpp:226] conv2 needs backward computation.
I0328 15:03:53.654948 163330 net.cpp:226] norm1 needs backward computation.
I0328 15:03:53.654953 163330 net.cpp:226] relu1 needs backward computation.
I0328 15:03:53.654958 163330 net.cpp:226] pool1 needs backward computation.
I0328 15:03:53.654963 163330 net.cpp:226] conv1 needs backward computation.
I0328 15:03:53.654970 163330 net.cpp:228] label_cifar_1_split does not need backward computation.
I0328 15:03:53.654978 163330 net.cpp:228] cifar does not need backward computation.
I0328 15:03:53.654981 163330 net.cpp:270] This network produces output accuracy
I0328 15:03:53.654990 163330 net.cpp:270] This network produces output loss
I0328 15:03:53.655028 163330 net.cpp:283] Network initialization done.
I0328 15:03:53.655104 163330 solver.cpp:60] Solver scaffolding done.
I0328 15:03:53.655194 163330 caffe.cpp:251] Starting Optimization
I0328 15:03:53.655200 163330 solver.cpp:279] Solving CIFAR10_full
I0328 15:03:53.655203 163330 solver.cpp:280] Learning Rate Policy: fixed
I0328 15:03:53.655443 163330 solver.cpp:337] Iteration 0, Testing net (#0)
OMP: Info #204: KMP_AFFINITY: decoding x2APIC ids.
OMP: Info #202: KMP_AFFINITY: Affinity capable, using global cpuid leaf 11 info
OMP: Info #154: KMP_AFFINITY: Initial OS proc set respected: {0,4,8,12,16,20,24,28,32,36,40,44,48,52,56,60,64,68,72,76,80,84,88,92,96,100,104,108}
OMP: Info #156: KMP_AFFINITY: 28 available OS procs
OMP: Info #157: KMP_AFFINITY: Uniform topology
OMP: Info #179: KMP_AFFINITY: 1 packages x 14 cores/pkg x 2 threads/core (14 total cores)
OMP: Info #206: KMP_AFFINITY: OS proc to physical thread map:
OMP: Info #171: KMP_AFFINITY: OS proc 0 maps to package 0 core 0 thread 0 
OMP: Info #171: KMP_AFFINITY: OS proc 56 maps to package 0 core 0 thread 1 
OMP: Info #171: KMP_AFFINITY: OS proc 4 maps to package 0 core 1 thread 0 
OMP: Info #171: KMP_AFFINITY: OS proc 60 maps to package 0 core 1 thread 1 
OMP: Info #171: KMP_AFFINITY: OS proc 8 maps to package 0 core 2 thread 0 
OMP: Info #171: KMP_AFFINITY: OS proc 64 maps to package 0 core 2 thread 1 
OMP: Info #171: KMP_AFFINITY: OS proc 12 maps to package 0 core 3 thread 0 
OMP: Info #171: KMP_AFFINITY: OS proc 68 maps to package 0 core 3 thread 1 
OMP: Info #171: KMP_AFFINITY: OS proc 16 maps to package 0 core 4 thread 0 
OMP: Info #171: KMP_AFFINITY: OS proc 72 maps to package 0 core 4 thread 1 
OMP: Info #171: KMP_AFFINITY: OS proc 20 maps to package 0 core 5 thread 0 
OMP: Info #171: KMP_AFFINITY: OS proc 76 maps to package 0 core 5 thread 1 
OMP: Info #171: KMP_AFFINITY: OS proc 24 maps to package 0 core 6 thread 0 
OMP: Info #171: KMP_AFFINITY: OS proc 80 maps to package 0 core 6 thread 1 
OMP: Info #171: KMP_AFFINITY: OS proc 28 maps to package 0 core 8 thread 0 
OMP: Info #171: KMP_AFFINITY: OS proc 84 maps to package 0 core 8 thread 1 
OMP: Info #171: KMP_AFFINITY: OS proc 32 maps to package 0 core 9 thread 0 
OMP: Info #171: KMP_AFFINITY: OS proc 88 maps to package 0 core 9 thread 1 
OMP: Info #171: KMP_AFFINITY: OS proc 36 maps to package 0 core 10 thread 0 
OMP: Info #171: KMP_AFFINITY: OS proc 92 maps to package 0 core 10 thread 1 
OMP: Info #171: KMP_AFFINITY: OS proc 40 maps to package 0 core 11 thread 0 
OMP: Info #171: KMP_AFFINITY: OS proc 96 maps to package 0 core 11 thread 1 
OMP: Info #171: KMP_AFFINITY: OS proc 44 maps to package 0 core 12 thread 0 
OMP: Info #171: KMP_AFFINITY: OS proc 100 maps to package 0 core 12 thread 1 
OMP: Info #171: KMP_AFFINITY: OS proc 48 maps to package 0 core 13 thread 0 
OMP: Info #171: KMP_AFFINITY: OS proc 104 maps to package 0 core 13 thread 1 
OMP: Info #171: KMP_AFFINITY: OS proc 52 maps to package 0 core 14 thread 0 
OMP: Info #171: KMP_AFFINITY: OS proc 108 maps to package 0 core 14 thread 1 
OMP: Info #144: KMP_AFFINITY: Threads may migrate across 1 innermost levels of machine
OMP: Info #242: KMP_AFFINITY: pid 163330 thread 0 bound to OS proc set {0,56}
OMP: Info #242: KMP_AFFINITY: pid 163330 thread 3 bound to OS proc set {12,68}
OMP: Info #242: KMP_AFFINITY: pid 163330 thread 2 bound to OS proc set {8,64}
OMP: Info #242: KMP_AFFINITY: pid 163330 thread 1 bound to OS proc set {4,60}
OMP: Info #242: KMP_AFFINITY: pid 163330 thread 4 bound to OS proc set {16,72}
OMP: Info #242: KMP_AFFINITY: pid 163330 thread 5 bound to OS proc set {20,76}
OMP: Info #242: KMP_AFFINITY: pid 163330 thread 6 bound to OS proc set {24,80}
OMP: Info #242: KMP_AFFINITY: pid 163330 thread 7 bound to OS proc set {28,84}
OMP: Info #242: KMP_AFFINITY: pid 163330 thread 8 bound to OS proc set {32,88}
OMP: Info #242: KMP_AFFINITY: pid 163330 thread 9 bound to OS proc set {36,92}
OMP: Info #242: KMP_AFFINITY: pid 163330 thread 10 bound to OS proc set {40,96}
OMP: Info #242: KMP_AFFINITY: pid 163330 thread 11 bound to OS proc set {44,100}
OMP: Info #242: KMP_AFFINITY: pid 163330 thread 12 bound to OS proc set {48,104}
OMP: Info #242: KMP_AFFINITY: pid 163330 thread 13 bound to OS proc set {52,108}
I0328 15:03:55.270195 163330 solver.cpp:228] Iteration 0, loss = 2.30265
I0328 15:03:55.270290 163330 solver.cpp:244]     Train net output #0: loss = 2.30265 (* 1 = 2.30265 loss)
I0328 15:03:55.270306 163330 sgd_solver.cpp:106] Iteration 0, lr = 0.001
I0328 15:24:17.167038 163330 solver.cpp:317] Iteration 800, loss = 1.77589
I0328 15:24:17.167168 163330 solver.cpp:322] Optimization Done.
I0328 15:24:17.167174 163330 caffe.cpp:254] Optimization Done.

 Performance counter stats for './build/tools/caffe.bin train --solver=examples/cifar10/cifar10_full_solver_200_0T_bsize112.prototxt':

       381,224,741      node-loads                                                  
         5,461,560      node-load-misses                                            

    1223.624633391 seconds time elapsed


real	20m23.633s
user	278m14.163s
sys	7m14.369s
