I0116 19:26:34.020391 20811 caffe.cpp:259] Use CPU.
I0116 19:26:34.021212 20811 solver.cpp:90] Initializing solver from parameters: 
test_iter: 0
test_interval: 1000
base_lr: 0.01
display: 20
max_iter: 20
lr_policy: "step"
gamma: 0.1
momentum: 0.9
weight_decay: 0.0005
stepsize: 100000
snapshot: 10000
snapshot_prefix: "models/bvlc_alexnet/caffe_alexnet_train"
solver_mode: CPU
net: "models/bvlc_alexnet/train_val.prototxt"
train_state {
  level: 0
  stage: ""
}
snapshot_after_train: false
I0116 19:26:34.021360 20811 solver.cpp:135] Creating training net from net file: models/bvlc_alexnet/train_val.prototxt
I0116 19:26:34.024808 20811 cpu_info.cpp:452] Processor speed [MHz]: 2700
I0116 19:26:34.024829 20811 cpu_info.cpp:455] Total number of sockets: 4
I0116 19:26:34.024838 20811 cpu_info.cpp:458] Total number of CPU cores: 32
I0116 19:26:34.024847 20811 cpu_info.cpp:461] Total number of processors: 64
I0116 19:26:34.024855 20811 cpu_info.cpp:464] GPU is used: no
I0116 19:26:34.024863 20811 cpu_info.cpp:467] OpenMP environmental variables are specified: yes
I0116 19:26:34.024873 20811 cpu_info.cpp:470] OpenMP thread bind allowed: no
OMP: Info #204: KMP_AFFINITY: decoding x2APIC ids.
OMP: Info #202: KMP_AFFINITY: Affinity capable, using global cpuid leaf 11 info
OMP: Info #154: KMP_AFFINITY: Initial OS proc set respected: {0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63}
OMP: Info #156: KMP_AFFINITY: 64 available OS procs
OMP: Info #157: KMP_AFFINITY: Uniform topology
OMP: Info #179: KMP_AFFINITY: 4 packages x 8 cores/pkg x 2 threads/core (32 total cores)
OMP: Info #206: KMP_AFFINITY: OS proc to physical thread map:
OMP: Info #171: KMP_AFFINITY: OS proc 0 maps to package 0 core 0 thread 0 
OMP: Info #171: KMP_AFFINITY: OS proc 32 maps to package 0 core 0 thread 1 
OMP: Info #171: KMP_AFFINITY: OS proc 1 maps to package 0 core 1 thread 0 
OMP: Info #171: KMP_AFFINITY: OS proc 33 maps to package 0 core 1 thread 1 
OMP: Info #171: KMP_AFFINITY: OS proc 2 maps to package 0 core 2 thread 0 
OMP: Info #171: KMP_AFFINITY: OS proc 34 maps to package 0 core 2 thread 1 
OMP: Info #171: KMP_AFFINITY: OS proc 3 maps to package 0 core 3 thread 0 
OMP: Info #171: KMP_AFFINITY: OS proc 35 maps to package 0 core 3 thread 1 
OMP: Info #171: KMP_AFFINITY: OS proc 4 maps to package 0 core 4 thread 0 
OMP: Info #171: KMP_AFFINITY: OS proc 36 maps to package 0 core 4 thread 1 
OMP: Info #171: KMP_AFFINITY: OS proc 5 maps to package 0 core 5 thread 0 
OMP: Info #171: KMP_AFFINITY: OS proc 37 maps to package 0 core 5 thread 1 
OMP: Info #171: KMP_AFFINITY: OS proc 6 maps to package 0 core 6 thread 0 
OMP: Info #171: KMP_AFFINITY: OS proc 38 maps to package 0 core 6 thread 1 
OMP: Info #171: KMP_AFFINITY: OS proc 7 maps to package 0 core 7 thread 0 
OMP: Info #171: KMP_AFFINITY: OS proc 39 maps to package 0 core 7 thread 1 
OMP: Info #171: KMP_AFFINITY: OS proc 8 maps to package 1 core 0 thread 0 
OMP: Info #171: KMP_AFFINITY: OS proc 40 maps to package 1 core 0 thread 1 
OMP: Info #171: KMP_AFFINITY: OS proc 9 maps to package 1 core 1 thread 0 
OMP: Info #171: KMP_AFFINITY: OS proc 41 maps to package 1 core 1 thread 1 
OMP: Info #171: KMP_AFFINITY: OS proc 10 maps to package 1 core 2 thread 0 
OMP: Info #171: KMP_AFFINITY: OS proc 42 maps to package 1 core 2 thread 1 
OMP: Info #171: KMP_AFFINITY: OS proc 11 maps to package 1 core 3 thread 0 
OMP: Info #171: KMP_AFFINITY: OS proc 43 maps to package 1 core 3 thread 1 
OMP: Info #171: KMP_AFFINITY: OS proc 12 maps to package 1 core 4 thread 0 
OMP: Info #171: KMP_AFFINITY: OS proc 44 maps to package 1 core 4 thread 1 
OMP: Info #171: KMP_AFFINITY: OS proc 13 maps to package 1 core 5 thread 0 
OMP: Info #171: KMP_AFFINITY: OS proc 45 maps to package 1 core 5 thread 1 
OMP: Info #171: KMP_AFFINITY: OS proc 14 maps to package 1 core 6 thread 0 
OMP: Info #171: KMP_AFFINITY: OS proc 46 maps to package 1 core 6 thread 1 
OMP: Info #171: KMP_AFFINITY: OS proc 15 maps to package 1 core 7 thread 0 
OMP: Info #171: KMP_AFFINITY: OS proc 47 maps to package 1 core 7 thread 1 
OMP: Info #171: KMP_AFFINITY: OS proc 16 maps to package 2 core 0 thread 0 
OMP: Info #171: KMP_AFFINITY: OS proc 48 maps to package 2 core 0 thread 1 
OMP: Info #171: KMP_AFFINITY: OS proc 17 maps to package 2 core 1 thread 0 
OMP: Info #171: KMP_AFFINITY: OS proc 49 maps to package 2 core 1 thread 1 
OMP: Info #171: KMP_AFFINITY: OS proc 18 maps to package 2 core 2 thread 0 
OMP: Info #171: KMP_AFFINITY: OS proc 50 maps to package 2 core 2 thread 1 
OMP: Info #171: KMP_AFFINITY: OS proc 19 maps to package 2 core 3 thread 0 
OMP: Info #171: KMP_AFFINITY: OS proc 51 maps to package 2 core 3 thread 1 
OMP: Info #171: KMP_AFFINITY: OS proc 20 maps to package 2 core 4 thread 0 
OMP: Info #171: KMP_AFFINITY: OS proc 52 maps to package 2 core 4 thread 1 
OMP: Info #171: KMP_AFFINITY: OS proc 21 maps to package 2 core 5 thread 0 
OMP: Info #171: KMP_AFFINITY: OS proc 53 maps to package 2 core 5 thread 1 
OMP: Info #171: KMP_AFFINITY: OS proc 22 maps to package 2 core 6 thread 0 
OMP: Info #171: KMP_AFFINITY: OS proc 54 maps to package 2 core 6 thread 1 
OMP: Info #171: KMP_AFFINITY: OS proc 23 maps to package 2 core 7 thread 0 
OMP: Info #171: KMP_AFFINITY: OS proc 55 maps to package 2 core 7 thread 1 
OMP: Info #171: KMP_AFFINITY: OS proc 24 maps to package 3 core 0 thread 0 
OMP: Info #171: KMP_AFFINITY: OS proc 56 maps to package 3 core 0 thread 1 
OMP: Info #171: KMP_AFFINITY: OS proc 25 maps to package 3 core 1 thread 0 
OMP: Info #171: KMP_AFFINITY: OS proc 57 maps to package 3 core 1 thread 1 
OMP: Info #171: KMP_AFFINITY: OS proc 26 maps to package 3 core 2 thread 0 
OMP: Info #171: KMP_AFFINITY: OS proc 58 maps to package 3 core 2 thread 1 
OMP: Info #171: KMP_AFFINITY: OS proc 27 maps to package 3 core 3 thread 0 
OMP: Info #171: KMP_AFFINITY: OS proc 59 maps to package 3 core 3 thread 1 
OMP: Info #171: KMP_AFFINITY: OS proc 28 maps to package 3 core 4 thread 0 
OMP: Info #171: KMP_AFFINITY: OS proc 60 maps to package 3 core 4 thread 1 
OMP: Info #171: KMP_AFFINITY: OS proc 29 maps to package 3 core 5 thread 0 
OMP: Info #171: KMP_AFFINITY: OS proc 61 maps to package 3 core 5 thread 1 
OMP: Info #171: KMP_AFFINITY: OS proc 30 maps to package 3 core 6 thread 0 
OMP: Info #171: KMP_AFFINITY: OS proc 62 maps to package 3 core 6 thread 1 
OMP: Info #171: KMP_AFFINITY: OS proc 31 maps to package 3 core 7 thread 0 
OMP: Info #171: KMP_AFFINITY: OS proc 63 maps to package 3 core 7 thread 1 
OMP: Info #144: KMP_AFFINITY: Threads may migrate across 1 innermost levels of machine
OMP: Info #242: KMP_AFFINITY: pid 20811 thread 0 bound to OS proc set {0,32}
I0116 19:26:34.031173 20811 cpu_info.cpp:473] Number of OpenMP threads: 32
I0116 19:26:34.031400 20811 net.cpp:484] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I0116 19:26:34.031432 20811 net.cpp:484] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0116 19:26:34.032697 20811 net.cpp:120] Initializing net from parameters: 
name: "AlexNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_file: "/home/user/caffeOMP/original/caffe/data/ilsvrc12/imagenet_mean.binaryproto"
  }
  data_param {
    source: "/home/user/caffeOMP/original/caffe/examples/imagenet/ilsvrc12_train_lmdb"
    batch_size: 256
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "norm1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "conv2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "norm2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8"
  bottom: "label"
  top: "loss"
}
I0116 19:26:34.032827 20811 layer_factory.hpp:114] Creating layer data
I0116 19:26:34.034528 20811 net.cpp:160] Creating Layer data
I0116 19:26:34.034605 20811 net.cpp:570] data -> data
I0116 19:26:34.034651 20811 net.cpp:570] data -> label
I0116 19:26:34.034680 20811 data_transformer.cpp:62] Loading mean file from: /home/user/caffeOMP/original/caffe/data/ilsvrc12/imagenet_mean.binaryproto
I0116 19:26:34.034883 20812 db_lmdb.cpp:72] Opened lmdb /home/user/caffeOMP/original/caffe/examples/imagenet/ilsvrc12_train_lmdb
I0116 19:26:34.046103 20811 data_layer.cpp:80] output data size: 256,3,227,227
I0116 19:26:34.312302 20811 net.cpp:210] Setting up data
I0116 19:26:34.312453 20811 net.cpp:217] Top shape: 256 3 227 227 (39574272)
I0116 19:26:34.312474 20811 net.cpp:217] Top shape: 256 (256)
I0116 19:26:34.312486 20811 net.cpp:225] Memory required for data: 158298112
I0116 19:26:34.312518 20811 layer_factory.hpp:114] Creating layer conv1
I0116 19:26:34.312603 20811 net.cpp:160] Creating Layer conv1
I0116 19:26:34.312626 20811 net.cpp:596] conv1 <- data
I0116 19:26:34.312659 20811 net.cpp:570] conv1 -> conv1
I0116 19:26:34.396036 20811 net.cpp:210] Setting up conv1
I0116 19:26:34.396183 20811 net.cpp:217] Top shape: 256 96 55 55 (74342400)
I0116 19:26:34.396199 20811 net.cpp:225] Memory required for data: 455667712
I0116 19:26:34.396275 20811 layer_factory.hpp:114] Creating layer relu1
I0116 19:26:34.396330 20811 net.cpp:160] Creating Layer relu1
I0116 19:26:34.396350 20811 net.cpp:596] relu1 <- conv1
I0116 19:26:34.396375 20811 net.cpp:557] relu1 -> conv1 (in-place)
I0116 19:26:34.396416 20811 net.cpp:210] Setting up relu1
I0116 19:26:34.396432 20811 net.cpp:217] Top shape: 256 96 55 55 (74342400)
I0116 19:26:34.396500 20811 net.cpp:225] Memory required for data: 753037312
I0116 19:26:34.396514 20811 layer_factory.hpp:114] Creating layer norm1
I0116 19:26:34.396545 20811 net.cpp:160] Creating Layer norm1
I0116 19:26:34.396559 20811 net.cpp:596] norm1 <- conv1
I0116 19:26:34.396579 20811 net.cpp:570] norm1 -> norm1
I0116 19:26:34.396623 20811 net.cpp:210] Setting up norm1
I0116 19:26:34.396642 20811 net.cpp:217] Top shape: 256 96 55 55 (74342400)
I0116 19:26:34.396656 20811 net.cpp:225] Memory required for data: 1050406912
I0116 19:26:34.396670 20811 layer_factory.hpp:114] Creating layer pool1
I0116 19:26:34.396848 20811 net.cpp:160] Creating Layer pool1
I0116 19:26:34.396868 20811 net.cpp:596] pool1 <- norm1
I0116 19:26:34.396888 20811 net.cpp:570] pool1 -> pool1
I0116 19:26:34.396930 20811 net.cpp:210] Setting up pool1
I0116 19:26:34.396950 20811 net.cpp:217] Top shape: 256 96 27 27 (17915904)
I0116 19:26:34.396962 20811 net.cpp:225] Memory required for data: 1122070528
I0116 19:26:34.396977 20811 layer_factory.hpp:114] Creating layer conv2
I0116 19:26:34.397032 20811 net.cpp:160] Creating Layer conv2
I0116 19:26:34.397047 20811 net.cpp:596] conv2 <- pool1
I0116 19:26:34.397065 20811 net.cpp:570] conv2 -> conv2
I0116 19:26:34.570845 20811 net.cpp:210] Setting up conv2
I0116 19:26:34.570998 20811 net.cpp:217] Top shape: 256 256 27 27 (47775744)
I0116 19:26:34.571017 20811 net.cpp:225] Memory required for data: 1313173504
I0116 19:26:34.571084 20811 layer_factory.hpp:114] Creating layer relu2
I0116 19:26:34.571130 20811 net.cpp:160] Creating Layer relu2
I0116 19:26:34.571149 20811 net.cpp:596] relu2 <- conv2
I0116 19:26:34.571177 20811 net.cpp:557] relu2 -> conv2 (in-place)
I0116 19:26:34.571215 20811 net.cpp:210] Setting up relu2
I0116 19:26:34.571233 20811 net.cpp:217] Top shape: 256 256 27 27 (47775744)
I0116 19:26:34.571247 20811 net.cpp:225] Memory required for data: 1504276480
I0116 19:26:34.571264 20811 layer_factory.hpp:114] Creating layer norm2
I0116 19:26:34.571291 20811 net.cpp:160] Creating Layer norm2
I0116 19:26:34.571303 20811 net.cpp:596] norm2 <- conv2
I0116 19:26:34.571324 20811 net.cpp:570] norm2 -> norm2
I0116 19:26:34.571370 20811 net.cpp:210] Setting up norm2
I0116 19:26:34.571386 20811 net.cpp:217] Top shape: 256 256 27 27 (47775744)
I0116 19:26:34.571400 20811 net.cpp:225] Memory required for data: 1695379456
I0116 19:26:34.571413 20811 layer_factory.hpp:114] Creating layer pool2
I0116 19:26:34.571547 20811 net.cpp:160] Creating Layer pool2
I0116 19:26:34.571597 20811 net.cpp:596] pool2 <- norm2
I0116 19:26:34.571617 20811 net.cpp:570] pool2 -> pool2
I0116 19:26:34.571652 20811 net.cpp:210] Setting up pool2
I0116 19:26:34.571669 20811 net.cpp:217] Top shape: 256 256 13 13 (11075584)
I0116 19:26:34.571682 20811 net.cpp:225] Memory required for data: 1739681792
I0116 19:26:34.571696 20811 layer_factory.hpp:114] Creating layer conv3
I0116 19:26:34.571740 20811 net.cpp:160] Creating Layer conv3
I0116 19:26:34.571753 20811 net.cpp:596] conv3 <- pool2
I0116 19:26:34.571776 20811 net.cpp:570] conv3 -> conv3
I0116 19:26:34.683426 20811 net.cpp:210] Setting up conv3
I0116 19:26:34.683549 20811 net.cpp:217] Top shape: 256 384 13 13 (16613376)
I0116 19:26:34.683558 20811 net.cpp:225] Memory required for data: 1806135296
I0116 19:26:34.683588 20811 layer_factory.hpp:114] Creating layer relu3
I0116 19:26:34.683617 20811 net.cpp:160] Creating Layer relu3
I0116 19:26:34.683629 20811 net.cpp:596] relu3 <- conv3
I0116 19:26:34.683643 20811 net.cpp:557] relu3 -> conv3 (in-place)
I0116 19:26:34.683665 20811 net.cpp:210] Setting up relu3
I0116 19:26:34.683675 20811 net.cpp:217] Top shape: 256 384 13 13 (16613376)
I0116 19:26:34.683681 20811 net.cpp:225] Memory required for data: 1872588800
I0116 19:26:34.683688 20811 layer_factory.hpp:114] Creating layer conv4
I0116 19:26:34.683714 20811 net.cpp:160] Creating Layer conv4
I0116 19:26:34.683722 20811 net.cpp:596] conv4 <- conv3
I0116 19:26:34.683734 20811 net.cpp:570] conv4 -> conv4
I0116 19:26:34.785997 20811 net.cpp:210] Setting up conv4
I0116 19:26:34.786077 20811 net.cpp:217] Top shape: 256 384 13 13 (16613376)
I0116 19:26:34.786082 20811 net.cpp:225] Memory required for data: 1939042304
I0116 19:26:34.786094 20811 layer_factory.hpp:114] Creating layer relu4
I0116 19:26:34.786105 20811 net.cpp:160] Creating Layer relu4
I0116 19:26:34.786113 20811 net.cpp:596] relu4 <- conv4
I0116 19:26:34.786126 20811 net.cpp:557] relu4 -> conv4 (in-place)
I0116 19:26:34.786136 20811 net.cpp:210] Setting up relu4
I0116 19:26:34.786145 20811 net.cpp:217] Top shape: 256 384 13 13 (16613376)
I0116 19:26:34.786151 20811 net.cpp:225] Memory required for data: 2005495808
I0116 19:26:34.786159 20811 layer_factory.hpp:114] Creating layer conv5
I0116 19:26:34.786171 20811 net.cpp:160] Creating Layer conv5
I0116 19:26:34.786180 20811 net.cpp:596] conv5 <- conv4
I0116 19:26:34.786190 20811 net.cpp:570] conv5 -> conv5
I0116 19:26:34.848734 20811 net.cpp:210] Setting up conv5
I0116 19:26:34.848750 20811 net.cpp:217] Top shape: 256 256 13 13 (11075584)
I0116 19:26:34.848757 20811 net.cpp:225] Memory required for data: 2049798144
I0116 19:26:34.848770 20811 layer_factory.hpp:114] Creating layer relu5
I0116 19:26:34.848783 20811 net.cpp:160] Creating Layer relu5
I0116 19:26:34.848789 20811 net.cpp:596] relu5 <- conv5
I0116 19:26:34.848800 20811 net.cpp:557] relu5 -> conv5 (in-place)
I0116 19:26:34.848810 20811 net.cpp:210] Setting up relu5
I0116 19:26:34.848819 20811 net.cpp:217] Top shape: 256 256 13 13 (11075584)
I0116 19:26:34.848825 20811 net.cpp:225] Memory required for data: 2094100480
I0116 19:26:34.848831 20811 layer_factory.hpp:114] Creating layer pool5
I0116 19:26:34.848884 20811 net.cpp:160] Creating Layer pool5
I0116 19:26:34.848893 20811 net.cpp:596] pool5 <- conv5
I0116 19:26:34.848903 20811 net.cpp:570] pool5 -> pool5
I0116 19:26:34.848923 20811 net.cpp:210] Setting up pool5
I0116 19:26:34.848933 20811 net.cpp:217] Top shape: 256 256 6 6 (2359296)
I0116 19:26:34.848939 20811 net.cpp:225] Memory required for data: 2103537664
I0116 19:26:34.848947 20811 layer_factory.hpp:114] Creating layer fc6
I0116 19:26:34.848970 20811 net.cpp:160] Creating Layer fc6
I0116 19:26:34.848978 20811 net.cpp:596] fc6 <- pool5
I0116 19:26:34.848997 20811 net.cpp:570] fc6 -> fc6
I0116 19:26:36.103194 20811 net.cpp:210] Setting up fc6
I0116 19:26:36.103348 20811 net.cpp:217] Top shape: 256 4096 (1048576)
I0116 19:26:36.103358 20811 net.cpp:225] Memory required for data: 2107731968
I0116 19:26:36.103385 20811 layer_factory.hpp:114] Creating layer relu6
I0116 19:26:36.103446 20811 net.cpp:160] Creating Layer relu6
I0116 19:26:36.103461 20811 net.cpp:596] relu6 <- fc6
I0116 19:26:36.103483 20811 net.cpp:557] relu6 -> fc6 (in-place)
I0116 19:26:36.103507 20811 net.cpp:210] Setting up relu6
I0116 19:26:36.103515 20811 net.cpp:217] Top shape: 256 4096 (1048576)
I0116 19:26:36.103523 20811 net.cpp:225] Memory required for data: 2111926272
I0116 19:26:36.103529 20811 layer_factory.hpp:114] Creating layer drop6
I0116 19:26:36.103554 20811 net.cpp:160] Creating Layer drop6
I0116 19:26:36.103560 20811 net.cpp:596] drop6 <- fc6
I0116 19:26:36.103569 20811 net.cpp:557] drop6 -> fc6 (in-place)
I0116 19:26:36.103590 20811 net.cpp:210] Setting up drop6
I0116 19:26:36.103598 20811 net.cpp:217] Top shape: 256 4096 (1048576)
I0116 19:26:36.103605 20811 net.cpp:225] Memory required for data: 2116120576
I0116 19:26:36.103612 20811 layer_factory.hpp:114] Creating layer fc7
I0116 19:26:36.103634 20811 net.cpp:160] Creating Layer fc7
I0116 19:26:36.103641 20811 net.cpp:596] fc7 <- fc6
I0116 19:26:36.103652 20811 net.cpp:570] fc7 -> fc7
I0116 19:26:36.671231 20811 net.cpp:210] Setting up fc7
I0116 19:26:36.671387 20811 net.cpp:217] Top shape: 256 4096 (1048576)
I0116 19:26:36.671397 20811 net.cpp:225] Memory required for data: 2120314880
I0116 19:26:36.671423 20811 layer_factory.hpp:114] Creating layer relu7
I0116 19:26:36.671458 20811 net.cpp:160] Creating Layer relu7
I0116 19:26:36.671473 20811 net.cpp:596] relu7 <- fc7
I0116 19:26:36.671491 20811 net.cpp:557] relu7 -> fc7 (in-place)
I0116 19:26:36.671514 20811 net.cpp:210] Setting up relu7
I0116 19:26:36.671566 20811 net.cpp:217] Top shape: 256 4096 (1048576)
I0116 19:26:36.671572 20811 net.cpp:225] Memory required for data: 2124509184
I0116 19:26:36.671579 20811 layer_factory.hpp:114] Creating layer drop7
I0116 19:26:36.671597 20811 net.cpp:160] Creating Layer drop7
I0116 19:26:36.671603 20811 net.cpp:596] drop7 <- fc7
I0116 19:26:36.671613 20811 net.cpp:557] drop7 -> fc7 (in-place)
I0116 19:26:36.671630 20811 net.cpp:210] Setting up drop7
I0116 19:26:36.671638 20811 net.cpp:217] Top shape: 256 4096 (1048576)
I0116 19:26:36.671645 20811 net.cpp:225] Memory required for data: 2128703488
I0116 19:26:36.671653 20811 layer_factory.hpp:114] Creating layer fc8
I0116 19:26:36.671679 20811 net.cpp:160] Creating Layer fc8
I0116 19:26:36.671685 20811 net.cpp:596] fc8 <- fc7
I0116 19:26:36.671696 20811 net.cpp:570] fc8 -> fc8
I0116 19:26:36.808321 20811 net.cpp:210] Setting up fc8
I0116 19:26:36.808380 20811 net.cpp:217] Top shape: 256 1000 (256000)
I0116 19:26:36.808387 20811 net.cpp:225] Memory required for data: 2129727488
I0116 19:26:36.808400 20811 layer_factory.hpp:114] Creating layer loss
I0116 19:26:36.808426 20811 net.cpp:160] Creating Layer loss
I0116 19:26:36.808435 20811 net.cpp:596] loss <- fc8
I0116 19:26:36.808444 20811 net.cpp:596] loss <- label
I0116 19:26:36.808455 20811 net.cpp:570] loss -> loss
I0116 19:26:36.808482 20811 layer_factory.hpp:114] Creating layer loss
OMP: Info #242: KMP_AFFINITY: pid 20811 thread 1 bound to OS proc set {8,40}
OMP: Info #242: KMP_AFFINITY: pid 20811 thread 2 bound to OS proc set {16,48}
OMP: Info #242: KMP_AFFINITY: pid 20811 thread 3 bound to OS proc set {24,56}
OMP: Info #242: KMP_AFFINITY: pid 20811 thread 4 bound to OS proc set {1,33}
OMP: Info #242: KMP_AFFINITY: pid 20811 thread 5 bound to OS proc set {9,41}
OMP: Info #242: KMP_AFFINITY: pid 20811 thread 6 bound to OS proc set {17,49}
OMP: Info #242: KMP_AFFINITY: pid 20811 thread 8 bound to OS proc set {2,34}
OMP: Info #242: KMP_AFFINITY: pid 20811 thread 7 bound to OS proc set {25,57}
OMP: Info #242: KMP_AFFINITY: pid 20811 thread 9 bound to OS proc set {10,42}
OMP: Info #242: KMP_AFFINITY: pid 20811 thread 10 bound to OS proc set {18,50}
OMP: Info #242: KMP_AFFINITY: pid 20811 thread 11 bound to OS proc set {26,58}
OMP: Info #242: KMP_AFFINITY: pid 20811 thread 12 bound to OS proc set {3,35}
OMP: Info #242: KMP_AFFINITY: pid 20811 thread 13 bound to OS proc set {11,43}
OMP: Info #242: KMP_AFFINITY: pid 20811 thread 14 bound to OS proc set {19,51}
OMP: Info #242: KMP_AFFINITY: pid 20811 thread 15 bound to OS proc set {27,59}
OMP: Info #242: KMP_AFFINITY: pid 20811 thread 17 bound to OS proc set {12,44}
OMP: Info #242: KMP_AFFINITY: pid 20811 thread 16 bound to OS proc set {4,36}
OMP: Info #242: KMP_AFFINITY: pid 20811 thread 18 bound to OS proc set {20,52}
OMP: Info #242: KMP_AFFINITY: pid 20811 thread 19 bound to OS proc set {28,60}
OMP: Info #242: KMP_AFFINITY: pid 20811 thread 20 bound to OS proc set {5,37}
OMP: Info #242: KMP_AFFINITY: pid 20811 thread 21 bound to OS proc set {13,45}
OMP: Info #242: KMP_AFFINITY: pid 20811 thread 22 bound to OS proc set {21,53}
OMP: Info #242: KMP_AFFINITY: pid 20811 thread 23 bound to OS proc set {29,61}
OMP: Info #242: KMP_AFFINITY: pid 20811 thread 24 bound to OS proc set {6,38}
OMP: Info #242: KMP_AFFINITY: pid 20811 thread 25 bound to OS proc set {14,46}
OMP: Info #242: KMP_AFFINITY: pid 20811 thread 26 bound to OS proc set {22,54}
OMP: Info #242: KMP_AFFINITY: pid 20811 thread 27 bound to OS proc set {30,62}
OMP: Info #242: KMP_AFFINITY: pid 20811 thread 30 bound to OS proc set {23,55}
OMP: Info #242: KMP_AFFINITY: pid 20811 thread 28 bound to OS proc set {7,39}
OMP: Info #242: KMP_AFFINITY: pid 20811 thread 29 bound to OS proc set {15,47}
OMP: Info #242: KMP_AFFINITY: pid 20811 thread 31 bound to OS proc set {31,63}
I0116 19:26:36.816287 20811 net.cpp:210] Setting up loss
I0116 19:26:36.816310 20811 net.cpp:217] Top shape: (1)
I0116 19:26:36.816318 20811 net.cpp:220]     with loss weight 1
I0116 19:26:36.816372 20811 net.cpp:225] Memory required for data: 2129727492
I0116 19:26:36.816381 20811 net.cpp:287] loss needs backward computation.
I0116 19:26:36.816391 20811 net.cpp:287] fc8 needs backward computation.
I0116 19:26:36.816400 20811 net.cpp:287] drop7 needs backward computation.
I0116 19:26:36.816406 20811 net.cpp:287] relu7 needs backward computation.
I0116 19:26:36.816416 20811 net.cpp:287] fc7 needs backward computation.
I0116 19:26:36.816423 20811 net.cpp:287] drop6 needs backward computation.
I0116 19:26:36.816431 20811 net.cpp:287] relu6 needs backward computation.
I0116 19:26:36.816438 20811 net.cpp:287] fc6 needs backward computation.
I0116 19:26:36.816447 20811 net.cpp:287] pool5 needs backward computation.
I0116 19:26:36.816453 20811 net.cpp:287] relu5 needs backward computation.
I0116 19:26:36.816462 20811 net.cpp:287] conv5 needs backward computation.
I0116 19:26:36.816468 20811 net.cpp:287] relu4 needs backward computation.
I0116 19:26:36.816476 20811 net.cpp:287] conv4 needs backward computation.
I0116 19:26:36.816484 20811 net.cpp:287] relu3 needs backward computation.
I0116 19:26:36.816491 20811 net.cpp:287] conv3 needs backward computation.
I0116 19:26:36.816498 20811 net.cpp:287] pool2 needs backward computation.
I0116 19:26:36.816506 20811 net.cpp:287] norm2 needs backward computation.
I0116 19:26:36.816514 20811 net.cpp:287] relu2 needs backward computation.
I0116 19:26:36.816521 20811 net.cpp:287] conv2 needs backward computation.
I0116 19:26:36.816529 20811 net.cpp:287] pool1 needs backward computation.
I0116 19:26:36.816536 20811 net.cpp:287] norm1 needs backward computation.
I0116 19:26:36.816543 20811 net.cpp:287] relu1 needs backward computation.
I0116 19:26:36.816550 20811 net.cpp:287] conv1 needs backward computation.
I0116 19:26:36.816560 20811 net.cpp:289] data does not need backward computation.
I0116 19:26:36.816570 20811 net.cpp:331] This network produces output loss
I0116 19:26:36.816592 20811 net.cpp:345] Network initialization done.
I0116 19:26:36.817951 20811 solver.cpp:225] Creating test net (#0) specified by net file: models/bvlc_alexnet/train_val.prototxt
I0116 19:26:36.817971 20811 cpu_info.cpp:452] Processor speed [MHz]: 2700
I0116 19:26:36.818061 20811 cpu_info.cpp:455] Total number of sockets: 4
I0116 19:26:36.818069 20811 cpu_info.cpp:458] Total number of CPU cores: 32
I0116 19:26:36.818075 20811 cpu_info.cpp:461] Total number of processors: 64
I0116 19:26:36.818083 20811 cpu_info.cpp:464] GPU is used: no
I0116 19:26:36.818126 20811 cpu_info.cpp:467] OpenMP environmental variables are specified: yes
I0116 19:26:36.818140 20811 cpu_info.cpp:470] OpenMP thread bind allowed: no
I0116 19:26:36.818146 20811 cpu_info.cpp:473] Number of OpenMP threads: 32
I0116 19:26:36.818220 20811 net.cpp:484] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I0116 19:26:36.819125 20811 net.cpp:120] Initializing net from parameters: 
name: "AlexNet"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mirror: false
    crop_size: 227
    mean_file: "/home/user/caffeOMP/original/caffe/data/ilsvrc12/imagenet_mean.binaryproto"
  }
  data_param {
    source: "/home/user/caffeOMP/original/caffe/examples/imagenet/ilsvrc12_val_lmdb"
    batch_size: 50
    backend: LMDB
  }
}
layer {
  name: "label_data_1_split"
  type: "Split"
  bottom: "label"
  top: "label_data_1_split_0"
  top: "label_data_1_split_1"
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "norm1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "conv2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "norm2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "fc8_fc8_0_split"
  type: "Split"
  bottom: "fc8"
  top: "fc8_fc8_0_split_0"
  top: "fc8_fc8_0_split_1"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8_fc8_0_split_0"
  bottom: "label_data_1_split_0"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8_fc8_0_split_1"
  bottom: "label_data_1_split_1"
  top: "loss"
}
I0116 19:26:36.819167 20811 layer_factory.hpp:114] Creating layer data
I0116 19:26:36.819298 20811 net.cpp:160] Creating Layer data
I0116 19:26:36.819319 20811 net.cpp:570] data -> data
I0116 19:26:36.819345 20811 net.cpp:570] data -> label
I0116 19:26:36.819370 20811 data_transformer.cpp:62] Loading mean file from: /home/user/caffeOMP/original/caffe/data/ilsvrc12/imagenet_mean.binaryproto
I0116 19:26:36.819509 20845 db_lmdb.cpp:72] Opened lmdb /home/user/caffeOMP/original/caffe/examples/imagenet/ilsvrc12_val_lmdb
I0116 19:26:36.825213 20811 data_layer.cpp:80] output data size: 50,3,227,227
I0116 19:26:36.867110 20811 net.cpp:210] Setting up data
I0116 19:26:36.867233 20811 net.cpp:217] Top shape: 50 3 227 227 (7729350)
I0116 19:26:36.867252 20811 net.cpp:217] Top shape: 50 (50)
I0116 19:26:36.867265 20811 net.cpp:225] Memory required for data: 30917600
I0116 19:26:36.867293 20811 layer_factory.hpp:114] Creating layer label_data_1_split
I0116 19:26:36.867342 20811 net.cpp:160] Creating Layer label_data_1_split
I0116 19:26:36.867360 20811 net.cpp:596] label_data_1_split <- label
I0116 19:26:36.867386 20811 net.cpp:570] label_data_1_split -> label_data_1_split_0
I0116 19:26:36.867419 20811 net.cpp:570] label_data_1_split -> label_data_1_split_1
I0116 19:26:36.867456 20811 net.cpp:210] Setting up label_data_1_split
I0116 19:26:36.867471 20811 net.cpp:217] Top shape: 50 (50)
I0116 19:26:36.867487 20811 net.cpp:217] Top shape: 50 (50)
I0116 19:26:36.867498 20811 net.cpp:225] Memory required for data: 30918000
I0116 19:26:36.867511 20811 layer_factory.hpp:114] Creating layer conv1
I0116 19:26:36.867553 20811 net.cpp:160] Creating Layer conv1
I0116 19:26:36.867565 20811 net.cpp:596] conv1 <- data
I0116 19:26:36.867583 20811 net.cpp:570] conv1 -> conv1
I0116 19:26:36.924597 20811 net.cpp:210] Setting up conv1
I0116 19:26:36.924728 20811 net.cpp:217] Top shape: 50 96 55 55 (14520000)
I0116 19:26:36.924737 20811 net.cpp:225] Memory required for data: 88998000
I0116 19:26:36.924779 20811 layer_factory.hpp:114] Creating layer relu1
I0116 19:26:36.924809 20811 net.cpp:160] Creating Layer relu1
I0116 19:26:36.924819 20811 net.cpp:596] relu1 <- conv1
I0116 19:26:36.924834 20811 net.cpp:557] relu1 -> conv1 (in-place)
I0116 19:26:36.924859 20811 net.cpp:210] Setting up relu1
I0116 19:26:36.924909 20811 net.cpp:217] Top shape: 50 96 55 55 (14520000)
I0116 19:26:36.924922 20811 net.cpp:225] Memory required for data: 147078000
I0116 19:26:36.924929 20811 layer_factory.hpp:114] Creating layer norm1
I0116 19:26:36.924952 20811 net.cpp:160] Creating Layer norm1
I0116 19:26:36.924959 20811 net.cpp:596] norm1 <- conv1
I0116 19:26:36.925062 20811 net.cpp:570] norm1 -> norm1
I0116 19:26:36.925092 20811 net.cpp:210] Setting up norm1
I0116 19:26:36.925102 20811 net.cpp:217] Top shape: 50 96 55 55 (14520000)
I0116 19:26:36.925109 20811 net.cpp:225] Memory required for data: 205158000
I0116 19:26:36.925117 20811 layer_factory.hpp:114] Creating layer pool1
I0116 19:26:36.925161 20811 net.cpp:160] Creating Layer pool1
I0116 19:26:36.925169 20811 net.cpp:596] pool1 <- norm1
I0116 19:26:36.925179 20811 net.cpp:570] pool1 -> pool1
I0116 19:26:36.925199 20811 net.cpp:210] Setting up pool1
I0116 19:26:36.925209 20811 net.cpp:217] Top shape: 50 96 27 27 (3499200)
I0116 19:26:36.925215 20811 net.cpp:225] Memory required for data: 219154800
I0116 19:26:36.925221 20811 layer_factory.hpp:114] Creating layer conv2
I0116 19:26:36.925248 20811 net.cpp:160] Creating Layer conv2
I0116 19:26:36.925256 20811 net.cpp:596] conv2 <- pool1
I0116 19:26:36.925266 20811 net.cpp:570] conv2 -> conv2
I0116 19:26:37.023705 20811 net.cpp:210] Setting up conv2
I0116 19:26:37.023725 20811 net.cpp:217] Top shape: 50 256 27 27 (9331200)
I0116 19:26:37.023732 20811 net.cpp:225] Memory required for data: 256479600
I0116 19:26:37.023746 20811 layer_factory.hpp:114] Creating layer relu2
I0116 19:26:37.023758 20811 net.cpp:160] Creating Layer relu2
I0116 19:26:37.023766 20811 net.cpp:596] relu2 <- conv2
I0116 19:26:37.023774 20811 net.cpp:557] relu2 -> conv2 (in-place)
I0116 19:26:37.023785 20811 net.cpp:210] Setting up relu2
I0116 19:26:37.023793 20811 net.cpp:217] Top shape: 50 256 27 27 (9331200)
I0116 19:26:37.023800 20811 net.cpp:225] Memory required for data: 293804400
I0116 19:26:37.023808 20811 layer_factory.hpp:114] Creating layer norm2
I0116 19:26:37.023819 20811 net.cpp:160] Creating Layer norm2
I0116 19:26:37.023826 20811 net.cpp:596] norm2 <- conv2
I0116 19:26:37.023836 20811 net.cpp:570] norm2 -> norm2
I0116 19:26:37.023850 20811 net.cpp:210] Setting up norm2
I0116 19:26:37.023859 20811 net.cpp:217] Top shape: 50 256 27 27 (9331200)
I0116 19:26:37.023866 20811 net.cpp:225] Memory required for data: 331129200
I0116 19:26:37.023874 20811 layer_factory.hpp:114] Creating layer pool2
I0116 19:26:37.023895 20811 net.cpp:160] Creating Layer pool2
I0116 19:26:37.023902 20811 net.cpp:596] pool2 <- norm2
I0116 19:26:37.023911 20811 net.cpp:570] pool2 -> pool2
I0116 19:26:37.023922 20811 net.cpp:210] Setting up pool2
I0116 19:26:37.023931 20811 net.cpp:217] Top shape: 50 256 13 13 (2163200)
I0116 19:26:37.023938 20811 net.cpp:225] Memory required for data: 339782000
I0116 19:26:37.023946 20811 layer_factory.hpp:114] Creating layer conv3
I0116 19:26:37.023958 20811 net.cpp:160] Creating Layer conv3
I0116 19:26:37.023967 20811 net.cpp:596] conv3 <- pool2
I0116 19:26:37.024070 20811 net.cpp:570] conv3 -> conv3
I0116 19:26:37.111428 20811 net.cpp:210] Setting up conv3
I0116 19:26:37.111479 20811 net.cpp:217] Top shape: 50 384 13 13 (3244800)
I0116 19:26:37.111486 20811 net.cpp:225] Memory required for data: 352761200
I0116 19:26:37.111508 20811 layer_factory.hpp:114] Creating layer relu3
I0116 19:26:37.111526 20811 net.cpp:160] Creating Layer relu3
I0116 19:26:37.111536 20811 net.cpp:596] relu3 <- conv3
I0116 19:26:37.111552 20811 net.cpp:557] relu3 -> conv3 (in-place)
I0116 19:26:37.111568 20811 net.cpp:210] Setting up relu3
I0116 19:26:37.111577 20811 net.cpp:217] Top shape: 50 384 13 13 (3244800)
I0116 19:26:37.111583 20811 net.cpp:225] Memory required for data: 365740400
I0116 19:26:37.111590 20811 layer_factory.hpp:114] Creating layer conv4
I0116 19:26:37.111608 20811 net.cpp:160] Creating Layer conv4
I0116 19:26:37.111615 20811 net.cpp:596] conv4 <- conv3
I0116 19:26:37.111627 20811 net.cpp:570] conv4 -> conv4
I0116 19:26:37.189667 20811 net.cpp:210] Setting up conv4
I0116 19:26:37.189718 20811 net.cpp:217] Top shape: 50 384 13 13 (3244800)
I0116 19:26:37.189724 20811 net.cpp:225] Memory required for data: 378719600
I0116 19:26:37.189735 20811 layer_factory.hpp:114] Creating layer relu4
I0116 19:26:37.189749 20811 net.cpp:160] Creating Layer relu4
I0116 19:26:37.189756 20811 net.cpp:596] relu4 <- conv4
I0116 19:26:37.189766 20811 net.cpp:557] relu4 -> conv4 (in-place)
I0116 19:26:37.189777 20811 net.cpp:210] Setting up relu4
I0116 19:26:37.189785 20811 net.cpp:217] Top shape: 50 384 13 13 (3244800)
I0116 19:26:37.189791 20811 net.cpp:225] Memory required for data: 391698800
I0116 19:26:37.189798 20811 layer_factory.hpp:114] Creating layer conv5
I0116 19:26:37.189813 20811 net.cpp:160] Creating Layer conv5
I0116 19:26:37.189821 20811 net.cpp:596] conv5 <- conv4
I0116 19:26:37.189831 20811 net.cpp:570] conv5 -> conv5
I0116 19:26:37.247366 20811 net.cpp:210] Setting up conv5
I0116 19:26:37.247419 20811 net.cpp:217] Top shape: 50 256 13 13 (2163200)
I0116 19:26:37.247434 20811 net.cpp:225] Memory required for data: 400351600
I0116 19:26:37.247452 20811 layer_factory.hpp:114] Creating layer relu5
I0116 19:26:37.247464 20811 net.cpp:160] Creating Layer relu5
I0116 19:26:37.247473 20811 net.cpp:596] relu5 <- conv5
I0116 19:26:37.247481 20811 net.cpp:557] relu5 -> conv5 (in-place)
I0116 19:26:37.247493 20811 net.cpp:210] Setting up relu5
I0116 19:26:37.247501 20811 net.cpp:217] Top shape: 50 256 13 13 (2163200)
I0116 19:26:37.247508 20811 net.cpp:225] Memory required for data: 409004400
I0116 19:26:37.247514 20811 layer_factory.hpp:114] Creating layer pool5
I0116 19:26:37.247560 20811 net.cpp:160] Creating Layer pool5
I0116 19:26:37.247570 20811 net.cpp:596] pool5 <- conv5
I0116 19:26:37.247580 20811 net.cpp:570] pool5 -> pool5
I0116 19:26:37.247598 20811 net.cpp:210] Setting up pool5
I0116 19:26:37.247608 20811 net.cpp:217] Top shape: 50 256 6 6 (460800)
I0116 19:26:37.247614 20811 net.cpp:225] Memory required for data: 410847600
I0116 19:26:37.247622 20811 layer_factory.hpp:114] Creating layer fc6
I0116 19:26:37.247643 20811 net.cpp:160] Creating Layer fc6
I0116 19:26:37.247650 20811 net.cpp:596] fc6 <- pool5
I0116 19:26:37.247661 20811 net.cpp:570] fc6 -> fc6
I0116 19:26:38.502480 20811 net.cpp:210] Setting up fc6
I0116 19:26:38.502626 20811 net.cpp:217] Top shape: 50 4096 (204800)
I0116 19:26:38.502635 20811 net.cpp:225] Memory required for data: 411666800
I0116 19:26:38.502661 20811 layer_factory.hpp:114] Creating layer relu6
I0116 19:26:38.502693 20811 net.cpp:160] Creating Layer relu6
I0116 19:26:38.502707 20811 net.cpp:596] relu6 <- fc6
I0116 19:26:38.502724 20811 net.cpp:557] relu6 -> fc6 (in-place)
I0116 19:26:38.502748 20811 net.cpp:210] Setting up relu6
I0116 19:26:38.502756 20811 net.cpp:217] Top shape: 50 4096 (204800)
I0116 19:26:38.502763 20811 net.cpp:225] Memory required for data: 412486000
I0116 19:26:38.502770 20811 layer_factory.hpp:114] Creating layer drop6
I0116 19:26:38.502796 20811 net.cpp:160] Creating Layer drop6
I0116 19:26:38.502804 20811 net.cpp:596] drop6 <- fc6
I0116 19:26:38.502812 20811 net.cpp:557] drop6 -> fc6 (in-place)
I0116 19:26:38.502828 20811 net.cpp:210] Setting up drop6
I0116 19:26:38.502836 20811 net.cpp:217] Top shape: 50 4096 (204800)
I0116 19:26:38.502843 20811 net.cpp:225] Memory required for data: 413305200
I0116 19:26:38.502851 20811 layer_factory.hpp:114] Creating layer fc7
I0116 19:26:38.502874 20811 net.cpp:160] Creating Layer fc7
I0116 19:26:38.502882 20811 net.cpp:596] fc7 <- fc6
I0116 19:26:38.502892 20811 net.cpp:570] fc7 -> fc7
I0116 19:26:39.062782 20811 net.cpp:210] Setting up fc7
I0116 19:26:39.062945 20811 net.cpp:217] Top shape: 50 4096 (204800)
I0116 19:26:39.062954 20811 net.cpp:225] Memory required for data: 414124400
I0116 19:26:39.062979 20811 layer_factory.hpp:114] Creating layer relu7
I0116 19:26:39.063022 20811 net.cpp:160] Creating Layer relu7
I0116 19:26:39.063036 20811 net.cpp:596] relu7 <- fc7
I0116 19:26:39.063055 20811 net.cpp:557] relu7 -> fc7 (in-place)
I0116 19:26:39.063122 20811 net.cpp:210] Setting up relu7
I0116 19:26:39.063132 20811 net.cpp:217] Top shape: 50 4096 (204800)
I0116 19:26:39.063144 20811 net.cpp:225] Memory required for data: 414943600
I0116 19:26:39.063151 20811 layer_factory.hpp:114] Creating layer drop7
I0116 19:26:39.063170 20811 net.cpp:160] Creating Layer drop7
I0116 19:26:39.063177 20811 net.cpp:596] drop7 <- fc7
I0116 19:26:39.063189 20811 net.cpp:557] drop7 -> fc7 (in-place)
I0116 19:26:39.063205 20811 net.cpp:210] Setting up drop7
I0116 19:26:39.063213 20811 net.cpp:217] Top shape: 50 4096 (204800)
I0116 19:26:39.063220 20811 net.cpp:225] Memory required for data: 415762800
I0116 19:26:39.063227 20811 layer_factory.hpp:114] Creating layer fc8
I0116 19:26:39.063249 20811 net.cpp:160] Creating Layer fc8
I0116 19:26:39.063256 20811 net.cpp:596] fc8 <- fc7
I0116 19:26:39.063266 20811 net.cpp:570] fc8 -> fc8
I0116 19:26:39.200623 20811 net.cpp:210] Setting up fc8
I0116 19:26:39.200685 20811 net.cpp:217] Top shape: 50 1000 (50000)
I0116 19:26:39.200693 20811 net.cpp:225] Memory required for data: 415962800
I0116 19:26:39.200706 20811 layer_factory.hpp:114] Creating layer fc8_fc8_0_split
I0116 19:26:39.200724 20811 net.cpp:160] Creating Layer fc8_fc8_0_split
I0116 19:26:39.200732 20811 net.cpp:596] fc8_fc8_0_split <- fc8
I0116 19:26:39.200744 20811 net.cpp:570] fc8_fc8_0_split -> fc8_fc8_0_split_0
I0116 19:26:39.200757 20811 net.cpp:570] fc8_fc8_0_split -> fc8_fc8_0_split_1
I0116 19:26:39.200772 20811 net.cpp:210] Setting up fc8_fc8_0_split
I0116 19:26:39.200781 20811 net.cpp:217] Top shape: 50 1000 (50000)
I0116 19:26:39.200789 20811 net.cpp:217] Top shape: 50 1000 (50000)
I0116 19:26:39.200795 20811 net.cpp:225] Memory required for data: 416362800
I0116 19:26:39.200803 20811 layer_factory.hpp:114] Creating layer accuracy
I0116 19:26:39.200827 20811 net.cpp:160] Creating Layer accuracy
I0116 19:26:39.200835 20811 net.cpp:596] accuracy <- fc8_fc8_0_split_0
I0116 19:26:39.200844 20811 net.cpp:596] accuracy <- label_data_1_split_0
I0116 19:26:39.200852 20811 net.cpp:570] accuracy -> accuracy
I0116 19:26:39.200870 20811 net.cpp:210] Setting up accuracy
I0116 19:26:39.200880 20811 net.cpp:217] Top shape: (1)
I0116 19:26:39.200886 20811 net.cpp:225] Memory required for data: 416362804
I0116 19:26:39.200892 20811 layer_factory.hpp:114] Creating layer loss
I0116 19:26:39.200908 20811 net.cpp:160] Creating Layer loss
I0116 19:26:39.200916 20811 net.cpp:596] loss <- fc8_fc8_0_split_1
I0116 19:26:39.200923 20811 net.cpp:596] loss <- label_data_1_split_1
I0116 19:26:39.200933 20811 net.cpp:570] loss -> loss
I0116 19:26:39.200951 20811 layer_factory.hpp:114] Creating layer loss
I0116 19:26:39.201236 20811 net.cpp:210] Setting up loss
I0116 19:26:39.201251 20811 net.cpp:217] Top shape: (1)
I0116 19:26:39.201256 20811 net.cpp:220]     with loss weight 1
I0116 19:26:39.201287 20811 net.cpp:225] Memory required for data: 416362808
I0116 19:26:39.201295 20811 net.cpp:287] loss needs backward computation.
I0116 19:26:39.201303 20811 net.cpp:289] accuracy does not need backward computation.
I0116 19:26:39.201310 20811 net.cpp:287] fc8_fc8_0_split needs backward computation.
I0116 19:26:39.201318 20811 net.cpp:287] fc8 needs backward computation.
I0116 19:26:39.201323 20811 net.cpp:287] drop7 needs backward computation.
I0116 19:26:39.201330 20811 net.cpp:287] relu7 needs backward computation.
I0116 19:26:39.201336 20811 net.cpp:287] fc7 needs backward computation.
I0116 19:26:39.201344 20811 net.cpp:287] drop6 needs backward computation.
I0116 19:26:39.201349 20811 net.cpp:287] relu6 needs backward computation.
I0116 19:26:39.201356 20811 net.cpp:287] fc6 needs backward computation.
I0116 19:26:39.201364 20811 net.cpp:287] pool5 needs backward computation.
I0116 19:26:39.201371 20811 net.cpp:287] relu5 needs backward computation.
I0116 19:26:39.201378 20811 net.cpp:287] conv5 needs backward computation.
I0116 19:26:39.201385 20811 net.cpp:287] relu4 needs backward computation.
I0116 19:26:39.201392 20811 net.cpp:287] conv4 needs backward computation.
I0116 19:26:39.201400 20811 net.cpp:287] relu3 needs backward computation.
I0116 19:26:39.201421 20811 net.cpp:287] conv3 needs backward computation.
I0116 19:26:39.201432 20811 net.cpp:287] pool2 needs backward computation.
I0116 19:26:39.201441 20811 net.cpp:287] norm2 needs backward computation.
I0116 19:26:39.201447 20811 net.cpp:287] relu2 needs backward computation.
I0116 19:26:39.201454 20811 net.cpp:287] conv2 needs backward computation.
I0116 19:26:39.201462 20811 net.cpp:287] pool1 needs backward computation.
I0116 19:26:39.201468 20811 net.cpp:287] norm1 needs backward computation.
I0116 19:26:39.201475 20811 net.cpp:287] relu1 needs backward computation.
I0116 19:26:39.201485 20811 net.cpp:287] conv1 needs backward computation.
I0116 19:26:39.201493 20811 net.cpp:289] label_data_1_split does not need backward computation.
I0116 19:26:39.201500 20811 net.cpp:289] data does not need backward computation.
I0116 19:26:39.201508 20811 net.cpp:331] This network produces output accuracy
I0116 19:26:39.201514 20811 net.cpp:331] This network produces output loss
I0116 19:26:39.201539 20811 net.cpp:345] Network initialization done.
I0116 19:26:39.201707 20811 solver.cpp:104] Solver scaffolding done.
I0116 19:26:39.201756 20811 caffe.cpp:310] Starting Optimization
I0116 19:26:39.201766 20811 solver.cpp:340] Solving AlexNet
I0116 19:26:39.201772 20811 solver.cpp:341] Learning Rate Policy: step
I0116 19:26:39.201781 20811 solver.cpp:406] Iteration 0, Testing net (#0)
I0116 19:26:45.484302 20811 solver.cpp:286] Iteration 0, loss = 6.90392
I0116 19:26:45.484446 20811 solver.cpp:303]     Train net output #0: loss = 6.90392 (* 1 = 6.90392 loss)
I0116 19:26:45.484472 20811 sgd_solver.cpp:143] Iteration 0, lr = 0.01
I0116 19:28:26.681511 20811 solver.cpp:382] Iteration 20, loss = 5.19525
I0116 19:28:26.681723 20811 solver.cpp:391] Optimization Done.
I0116 19:28:26.681733 20811 caffe.cpp:313] Optimization Done.

 Performance counter stats for '/home/user/caffeOMP/caffe_intel/self_containted_MKLGOLD_u1/caffe-self_containted_MKLGOLD_u1/build/tools/caffe.bin train --solver=models/bvlc_alexnet/solver_cust.prototxt':

     7,285,348,111      node-loads                                                  
     5,258,924,701      node-load-misses                                            

     112.818404292 seconds time elapsed


real	1m52.832s
user	56m58.651s
sys	0m30.046s
